{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58cKYP1AUuXx"
      },
      "source": [
        "# **Library and Dataset :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d71a7HeTzr3"
      },
      "source": [
        "## **Installation of PySpark :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt_5HfnlTnp4",
        "outputId": "077ad240-dca7-4198-d37e-49547fe990a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.9/dist-packages (3.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXlvt1VcT80E"
      },
      "source": [
        "## **Importation of PySpark :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIvvcSy0T_Hs"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqbkn2Nodj68"
      },
      "source": [
        "## **Importation of the dataset :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "oRxun68tdnUV",
        "outputId": "cdc0647f-cf97-455d-df4b-bcff2371b46c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1-Upload a file (If you are using Colab)\n",
            "2-Load files\n",
            "3-Leave\n",
            "Make your choice1\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bf8d1263-e353-4fd6-85a9-b2bb76ed012f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bf8d1263-e353-4fd6-85a9-b2bb76ed012f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving property_data.csv to property_data (1).csv\n",
            "Saving property_data_for_fusion.csv to property_data_for_fusion (1).csv\n",
            "1-Upload a file (If you are using Colab)\n",
            "2-Load files\n",
            "3-Leave\n",
            "Make your choice2\n",
            "Enter file locationproperty_data.csv\n",
            "Enter fusion file locationproperty_data_for_fusion.csv\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Import CSV\").getOrCreate()\n",
        "rep=True\n",
        "while(rep==True):\n",
        "  print(\"1-Upload a file (If you are using Colab)\")\n",
        "  print(\"2-Load files\")\n",
        "  print(\"3-Leave\")\n",
        "  n=int(input(\"Make your choice\"))\n",
        "  if(n==1):\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "  if(n==2):\n",
        "    f_name=input(\"Enter file location\")\n",
        "    df = spark.read.csv(f_name, header=True, inferSchema=True)\n",
        "    f_name_fusion=input(\"Enter fusion file location\")\n",
        "    df_fusion = spark.read.csv(f_name_fusion, header=True, inferSchema=True)\n",
        "    rep=False\n",
        "  if(n==3):\n",
        "    rep=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj7YnsTf4dnL"
      },
      "source": [
        "### **Replacing different kind of NaN typing values to None :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZGF5Yr32-a_"
      },
      "outputs": [],
      "source": [
        "df = df.replace([\"NA\",\"NaN\",\"na\",\"n/a\",\"--\"], None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbfuFB72Ug8I"
      },
      "source": [
        "# **Dataset Generalisation :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBy8U2wLZKe-"
      },
      "source": [
        "## **Library Importation :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2KrOD6dZPQ3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyspark.sql.functions import sum, avg, when, isnull, col, row_number, lit\n",
        "from pyspark.sql import Window\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF24LAnUZUam"
      },
      "source": [
        "## **Number of NaN values :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "sT63uzEfZ_5H",
        "outputId": "d7d5992e-0e37-4ed7-94a2-ad2190c157be"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-03292075-b068-4e04-81c1-1ac296cbe7db\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Number of NaN</th>\n",
              "      <th>Percentage</th>\n",
              "      <th>Type</th>\n",
              "      <th>Method</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>PID</th>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>INT</td>\n",
              "      <td>DROP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ST_NUM</th>\n",
              "      <td>2</td>\n",
              "      <td>22.222222</td>\n",
              "      <td>STRING</td>\n",
              "      <td>COUNT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ST_NAME</th>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>STRING</td>\n",
              "      <td>DROP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>OWN_OCCUPIED</th>\n",
              "      <td>1</td>\n",
              "      <td>11.111111</td>\n",
              "      <td>STRING</td>\n",
              "      <td>AVG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NUM_BEDROOMS</th>\n",
              "      <td>3</td>\n",
              "      <td>33.333333</td>\n",
              "      <td>STRING</td>\n",
              "      <td>COUNT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NUM_BATH</th>\n",
              "      <td>1</td>\n",
              "      <td>11.111111</td>\n",
              "      <td>STRING</td>\n",
              "      <td>AVG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SQ_FT</th>\n",
              "      <td>2</td>\n",
              "      <td>22.222222</td>\n",
              "      <td>STRING</td>\n",
              "      <td>COUNT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03292075-b068-4e04-81c1-1ac296cbe7db')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-03292075-b068-4e04-81c1-1ac296cbe7db button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-03292075-b068-4e04-81c1-1ac296cbe7db');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "              Number of NaN  Percentage    Type Method\n",
              "PID                       0    0.000000     INT   DROP\n",
              "ST_NUM                    2   22.222222  STRING  COUNT\n",
              "ST_NAME                   0    0.000000  STRING   DROP\n",
              "OWN_OCCUPIED              1   11.111111  STRING    AVG\n",
              "NUM_BEDROOMS              3   33.333333  STRING  COUNT\n",
              "NUM_BATH                  1   11.111111  STRING    AVG\n",
              "SQ_FT                     2   22.222222  STRING  COUNT"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Num_NaN=df.select(*[sum(df[col].isNull().cast(\"int\")).alias(col) for col in df.columns]).toPandas()\n",
        "Num_NaN.index=[\"Number of NaN\"]\n",
        "Num_NaN=Num_NaN.T\n",
        "Num_NaN[\"Percentage\"]=Num_NaN[\"Number of NaN\"]*100/df.count()\n",
        "Num_NaN[\"Type\"]=[df.dtypes[k][1].upper() for k in range(len(df.dtypes))]\n",
        "Num_NaN['Method'] = np.where(Num_NaN['Percentage'] < 10, 'DROP', np.where(Num_NaN['Percentage'] >= 20, 'COUNT', 'AVG'))\n",
        "Num_NaN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wIUp6JeaXM5"
      },
      "source": [
        "### **Quick look at our dataset's first rows :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qV4-AmGQz-ST",
        "outputId": "cb169d4f-cae6-4021-ecab-060bed20bb7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+------+----------+------------+------------+--------+-----+\n",
            "|  PID|ST_NUM|   ST_NAME|OWN_OCCUPIED|NUM_BEDROOMS|NUM_BATH|SQ_FT|\n",
            "+-----+------+----------+------------+------------+--------+-----+\n",
            "|10001|   104|    PUTNAM|           Y|           3|       1| 1000|\n",
            "|10002|   197| LEXINGTON|           N|           3|     1.5| null|\n",
            "|10003|  null| LEXINGTON|           N|        null|       1|  850|\n",
            "|10004|   201|  BERKELEY|          12|           1|    null|  700|\n",
            "|10005|   203|  BERKELEY|           Y|           3|       2| 1600|\n",
            "|10006|   207|  BERKELEY|           Y|        null|       1|  800|\n",
            "|10007|  null|WASHINGTON|        null|           2|  HURLEY|  950|\n",
            "|10008|   213|   TREMONT|           Y|           1|       1| null|\n",
            "|10009|   215|   TREMONT|           Y|        null|       2| 1800|\n",
            "+-----+------+----------+------------+------------+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4TrUtLre_1l"
      },
      "source": [
        "# **Data Cleaning :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcipJabIfNK-"
      },
      "source": [
        "## **Replacing NaN Values :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I7k8TaMgK1E"
      },
      "source": [
        "### **Creating List with Columns Methods of cleaning :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ_xvjf9gK1E",
        "outputId": "638838a0-1474-46f2-a05f-d3e68ed68b90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['PID', 'DROP', 'INT'], ['ST_NUM', 'COUNT', 'STRING'], ['ST_NAME', 'DROP', 'STRING'], ['OWN_OCCUPIED', 'AVG', 'STRING'], ['NUM_BEDROOMS', 'COUNT', 'STRING'], ['NUM_BATH', 'AVG', 'STRING'], ['SQ_FT', 'COUNT', 'STRING']]\n"
          ]
        }
      ],
      "source": [
        "L_Index=list(Num_NaN[\"Method\"].index)\n",
        "L_Method=list(Num_NaN[\"Method\"])\n",
        "L_Type=list(Num_NaN[\"Type\"])\n",
        "LF_Method=[]\n",
        "for k in range(len(L_Index)):\n",
        "    LF_Method.append([L_Index[k],L_Method[k],L_Type[k]])\n",
        "print(LF_Method)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "783-IaHFgK1E"
      },
      "source": [
        "### **Dropping NaN Values :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKLrii4XgK1F"
      },
      "outputs": [],
      "source": [
        "for k in range(len(LF_Method)):\n",
        "    if(LF_Method[k][1]==\"DROP\"):\n",
        "        df=df.dropna(subset=[LF_Method[k][0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRNnAgILqYRY"
      },
      "source": [
        "### **Creating lists containing distinct values of each columns of the dataset :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Znfq0vKm4fe"
      },
      "outputs": [],
      "source": [
        "L_Distinct=[]\n",
        "L_Distinct_NA=[]\n",
        "#à modifier (boucle for trop lourd)\n",
        "for k in range(len(LF_Method)):\n",
        "  M=[row[LF_Method[k][0]] for row in df.select(LF_Method[k][0]).distinct().collect()]\n",
        "  my_list = [x for x in M if x not in [None]]\n",
        "  L_Distinct.append(my_list)\n",
        "  L_Distinct_NA.append(M)\n",
        "L=[]\n",
        "#à modifier (boucle for trop lourd)\n",
        "for k in range(len(LF_Method)):\n",
        "  M=[row[LF_Method[k][0]] for row in df.select(LF_Method[k][0]).collect()]\n",
        "  my_list = [x for x in M if x not in [None]]\n",
        "  L.append(my_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZP3osDbgK1F"
      },
      "source": [
        "### **Replacing NaN Values by the AVG (if the column type is a number) and by a Random Value from the following column (if the column type isn't a number) :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox2Z5zAwgK1F"
      },
      "outputs": [],
      "source": [
        "for k in range(len(LF_Method)):\n",
        "  if(LF_Method[k][2]==\"INT\" or LF_Method[k][2]==\"DOUBLE\" or LF_Method[k][2]==\"FLOAT\"):\n",
        "    if(LF_Method[k][1]==\"AVG\"):\n",
        "        mean_col = df.agg(avg(LF_Method[k][0])).first()[0]\n",
        "        df = df.withColumn(LF_Method[k][0], when(isnull(LF_Method[k][0]), mean_col).otherwise(col(LF_Method[k][0])))\n",
        "  else:\n",
        "    if(LF_Method[k][1]==\"AVG\"):\n",
        "      n=np.random.randint(len(L_Distinct[k])+1)\n",
        "      df = df.withColumn(LF_Method[k][0], when(isnull(LF_Method[k][0]), L_Distinct[k][n]).otherwise(col(LF_Method[k][0])))\n",
        "    elif(LF_Method[k][1]==\"COUNT\"):\n",
        "      occurrence = []\n",
        "      for l in L:\n",
        "        occurrences = {}\n",
        "        for i in l:\n",
        "          occurrences[i] = l.count(i)\n",
        "        occurrence.append(occurrences)\n",
        "      max_count = max(occurrence[k].values())\n",
        "      for cle, valeur in occurrence[k].items():\n",
        "          if valeur == max_count:\n",
        "              max_count_index = cle\n",
        "      df = df.withColumn(LF_Method[k][0], when(isnull(LF_Method[k][0]), max_count_index).otherwise(col(LF_Method[k][0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyd_at82moI9"
      },
      "source": [
        "### **Quick look at the cleaned dataset :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Epfh9JONBK6S",
        "outputId": "7b59736c-8dbd-4566-8a8e-03a2e77dd3e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+------+----------+------------+------------+--------+-----+\n",
            "|  PID|ST_NUM|   ST_NAME|OWN_OCCUPIED|NUM_BEDROOMS|NUM_BATH|SQ_FT|\n",
            "+-----+------+----------+------------+------------+--------+-----+\n",
            "|10001|   104|    PUTNAM|           Y|           3|       1| 1000|\n",
            "|10002|   197| LEXINGTON|           N|           3|     1.5| 1800|\n",
            "|10003|   215| LEXINGTON|           N|           3|       1|  850|\n",
            "|10004|   201|  BERKELEY|          12|           1|  HURLEY|  700|\n",
            "|10005|   203|  BERKELEY|           Y|           3|       2| 1600|\n",
            "|10006|   207|  BERKELEY|           Y|           3|       1|  800|\n",
            "|10007|   215|WASHINGTON|           Y|           2|  HURLEY|  950|\n",
            "|10008|   213|   TREMONT|           Y|           1|       1| 1800|\n",
            "|10009|   215|   TREMONT|           Y|           3|       2| 1800|\n",
            "+-----+------+----------+------------+------------+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI54YUP_Py_r"
      },
      "source": [
        "# **Data Fusion :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhGwUiGdgcoT"
      },
      "source": [
        "### **Adding IDs :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogAc-RXLgcDx",
        "outputId": "8ae0fff3-0483-4ebe-c922-cc1129951d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type the column name of the primary keyPID\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "dataset_ID=input(\"type the column name of the primary key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKVR8yEJXcr4"
      },
      "source": [
        "### **Transforming our dataframe into temporary table :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8tPQ3pAXlTA"
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"my_table\")\n",
        "df_fusion.createOrReplaceTempView(\"my_table_fusion\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNgNklKCJYr5"
      },
      "source": [
        "### **Creating a list containing the fields where the fusion can be delicate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWVfrRdBJY9-"
      },
      "outputs": [],
      "source": [
        "Fields=[\"finance\",\"bank\",\"banks\",\"insurance\",\"health\",\"medical\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHRI9fRMQbKS"
      },
      "source": [
        "### **Creating a BackEnd DataFrame :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZHM2-OSQaaI"
      },
      "outputs": [],
      "source": [
        "df_Bis=df.toPandas()\n",
        "df_BackEnd=spark.createDataFrame(df_Bis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRqaq1vyIpRf"
      },
      "source": [
        "### **Adding the row number :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyHGNW12etv_"
      },
      "outputs": [],
      "source": [
        "df_BackEnd = df_BackEnd.withColumn(\"row_number\", row_number().over(Window.orderBy(dataset_ID)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYjik2J1EdF9"
      },
      "source": [
        "### **Adding the current date :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkPUCASSEcvb"
      },
      "outputs": [],
      "source": [
        "df_BackEnd = df_BackEnd.withColumn(\"modified_date\", lit(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afcxiX_JJPRy"
      },
      "source": [
        "### **Creating the following sql table :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFuNtdAzJPiy"
      },
      "outputs": [],
      "source": [
        "df_BackEnd.createOrReplaceTempView(\"my_table_BackEnd\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV92xwz2gbnH"
      },
      "source": [
        "### **Fusion Algorithm :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCqhKkeX4IPu"
      },
      "outputs": [],
      "source": [
        "def Fusion():\n",
        "  global res_fusion\n",
        "  res_fusion=df\n",
        "  global res_fusion_BackEnd\n",
        "  res_fusion_BackEnd=df_BackEnd\n",
        "  fusion_ids=[]\n",
        "  for k in range(len(spark.sql(\"select \"+dataset_ID+\" from my_table_fusion\").collect())):\n",
        "    fusion_ids.append(spark.sql(\"select \"+dataset_ID+\" from my_table_fusion\").collect()[k][0])\n",
        "  rep=True\n",
        "  while(rep==True):\n",
        "    print(\"1-Make a fusion\")\n",
        "    print(\"2-Leave\")\n",
        "    n1=int(input(\"Make your decision : \"))\n",
        "    if(n1==1):\n",
        "      # Asking for the field of study in order to know if the fusion could be delicate or no :\n",
        "      rep_type=input(\"Enter the field of study for this dataset\")\n",
        "      if(rep_type.lower() in Fields):\n",
        "        # Asking to consider the risk of doing fusion in   such a dataset :\n",
        "        rep_Bis=input(\"Your dataset could contain delicate data. Are you sure that you still want to continue the fusion process? (Y/N)\")\n",
        "        if(rep_Bis.upper()=='Y'):\n",
        "          Columns_to_be_Changed=res_fusion.columns[1:]\n",
        "          number=len(Columns_to_be_Changed)\n",
        "          for i in range(len(fusion_ids)):\n",
        "            id=fusion_ids[i]\n",
        "            Changes=[]\n",
        "            Current_Val=[]\n",
        "            for k in range(number):\n",
        "              col_type=\"\"\n",
        "              # Asking for the name of the columns to be added in the 'Columns_to_be_Changed' list :\n",
        "              for i1 in LF_Method:\n",
        "                if Columns_to_be_Changed[k] in i1:\n",
        "                  col_type=i1[2]\n",
        "              # Adding the following values in the lists we created previously :\n",
        "              if(col_type==\"INT\" or col_type==\"DOUBLE\" or col_type==\"FLOAT\"):\n",
        "                current_value=spark.sql(\"select \"+Columns_to_be_Changed[k]+\" from my_table Where \"+dataset_ID+\"=='\"+str(id)+\"'\").first()[0]\n",
        "                changed_value=spark.sql(\"select \"+Columns_to_be_Changed[k]+\" from my_table_fusion Where \"+dataset_ID+\"=='\"+str(id)+\"'\").first()[0]\n",
        "                Current_Val.append(current_value)\n",
        "                Changes.append(changed_value)\n",
        "              else:\n",
        "                current_value=spark.sql(\"select \"+Columns_to_be_Changed[k]+\" from my_table Where \"+dataset_ID+\"=='\"+str(id)+\"'\").first()[0]\n",
        "                changed_value=spark.sql(\"select \"+Columns_to_be_Changed[k]+\" from my_table_fusion Where \"+dataset_ID+\"=='\"+str(id)+\"'\").first()[0]\n",
        "                Current_Val.append(current_value)\n",
        "                Changes.append(changed_value)\n",
        "            # Row to duplicate in the BackEnd Dataset : \n",
        "            row_to_duplicate = res_fusion_BackEnd.filter(res_fusion_BackEnd[dataset_ID] == id).limit(1)\n",
        "            res_fusion_BackEnd = res_fusion_BackEnd.union(row_to_duplicate)\n",
        "            res_fusion_BackEnd = res_fusion_BackEnd.withColumn(\"row_number\", row_number().over(Window.orderBy(dataset_ID)))\n",
        "            res_fusion_BackEnd.createOrReplaceTempView(\"my_table_BackEnd\")\n",
        "            # Generating a list comporting the row numbers of each line in the BackEnd Dataset :\n",
        "            request_BackEnd=spark.sql(\"select row_number from my_table_BackEnd where \"+dataset_ID+\"=\"+str(id))\n",
        "            L_Index_BackEnd=[]\n",
        "            for k in range(request_BackEnd.count()):\n",
        "              L_Index_BackEnd.append(request_BackEnd.collect()[k][0])\n",
        "            # Replacing the line values in the visible Dataset and adding the line to the BackEnd Dataset :\n",
        "            for j in range(len(Columns_to_be_Changed)):\n",
        "              res_fusion=res_fusion.withColumn(Columns_to_be_Changed[j], when(res_fusion[dataset_ID]==id,Changes[j]).otherwise(res_fusion[Columns_to_be_Changed[j]]))\n",
        "              res_fusion_BackEnd=res_fusion_BackEnd.withColumn(Columns_to_be_Changed[j], when(res_fusion_BackEnd[\"row_number\"]==L_Index_BackEnd[-1],Changes[j]).otherwise(res_fusion_BackEnd[Columns_to_be_Changed[j]]))\n",
        "            res_fusion_BackEnd = res_fusion_BackEnd.withColumn(\"modified_date\", when(res_fusion_BackEnd[\"row_number\"]==L_Index_BackEnd[-1],lit(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))).otherwise(res_fusion_BackEnd[\"modified_date\"]))\n",
        "          rep=False\n",
        "        else:\n",
        "          rep=False\n",
        "      else:\n",
        "        Columns_to_be_Changed=res_fusion.columns[1:]\n",
        "        number=len(Columns_to_be_Changed)\n",
        "        for i in range(len(fusion_ids)):\n",
        "          id=fusion_ids[i]\n",
        "          Changes=[]\n",
        "          Current_Val=[]\n",
        "          for k in range(number):\n",
        "            col_type=\"\"\n",
        "            # Asking for the name of the columns to be added in the 'Columns_to_be_Changed' list :\n",
        "            for i1 in LF_Method:\n",
        "              if Columns_to_be_Changed[k] in i1:\n",
        "                col_type=i1[2]\n",
        "            # Adding the following values in the lists we created previously :\n",
        "            if(col_type==\"INT\" or col_type==\"DOUBLE\" or col_type==\"FLOAT\"):\n",
        "              current_value=spark.sql(\"select \"+Columns_to_be_Changed[k]+\" from my_table Where \"+dataset_ID+\"=='\"+str(id)+\"'\").first()[0]\n",
        "              changed_value=spark.sql(\"select \"+Columns_to_be_Changed[k]+\" from my_table_fusion Where \"+dataset_ID+\"=='\"+str(id)+\"'\").first()[0]\n",
        "              Current_Val.append(current_value)\n",
        "              Changes.append(changed_value)\n",
        "            else:\n",
        "              current_value=spark.sql(\"select \"+Columns_to_be_Changed[k]+\" from my_table Where \"+dataset_ID+\"=='\"+str(id)+\"'\").first()[0]\n",
        "              changed_value=spark.sql(\"select \"+Columns_to_be_Changed[k]+\" from my_table_fusion Where \"+dataset_ID+\"=='\"+str(id)+\"'\").first()[0]\n",
        "              Current_Val.append(current_value)\n",
        "              Changes.append(changed_value)\n",
        "          # Row to duplicate in the BackEnd Dataset : \n",
        "          row_to_duplicate = res_fusion_BackEnd.filter(res_fusion_BackEnd[dataset_ID] == id).limit(1)\n",
        "          res_fusion_BackEnd = res_fusion_BackEnd.union(row_to_duplicate)\n",
        "          res_fusion_BackEnd = res_fusion_BackEnd.withColumn(\"row_number\", row_number().over(Window.orderBy(dataset_ID)))\n",
        "          res_fusion_BackEnd.createOrReplaceTempView(\"my_table_BackEnd\")\n",
        "          # Generating a list comporting the row numbers of each line in the BackEnd Dataset :\n",
        "          request_BackEnd=spark.sql(\"select row_number from my_table_BackEnd where \"+dataset_ID+\"=\"+str(id))\n",
        "          L_Index_BackEnd=[]\n",
        "          for k in range(request_BackEnd.count()):\n",
        "            L_Index_BackEnd.append(request_BackEnd.collect()[k][0])\n",
        "          # Replacing the line values in the visible Dataset and adding the line to the BackEnd Dataset :\n",
        "          for j in range(len(Columns_to_be_Changed)):\n",
        "            res_fusion=res_fusion.withColumn(Columns_to_be_Changed[j], when(res_fusion[dataset_ID]==id,Changes[j]).otherwise(res_fusion[Columns_to_be_Changed[j]]))\n",
        "            res_fusion_BackEnd=res_fusion_BackEnd.withColumn(Columns_to_be_Changed[j], when(res_fusion_BackEnd[\"row_number\"]==L_Index_BackEnd[-1],Changes[j]).otherwise(res_fusion_BackEnd[Columns_to_be_Changed[j]]))\n",
        "          res_fusion_BackEnd = res_fusion_BackEnd.withColumn(\"modified_date\", when(res_fusion_BackEnd[\"row_number\"]==L_Index_BackEnd[-1],lit(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))).otherwise(res_fusion_BackEnd[\"modified_date\"]))\n",
        "        rep=False\n",
        "    if(n1==2):\n",
        "      rep=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CviPxNGcNWoQ"
      },
      "source": [
        "### **Making the fusion :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTStPmfuTCGr",
        "outputId": "6683174c-41d6-4621-c84c-d1e47cdeecd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1-Make a fusion\n",
            "2-Leave\n",
            "Make your decision : 1\n",
            "Enter the field of study for this datasetfinance\n",
            "Your dataset could contain delicate data. Are you sure that you still want to continue the fusion process? (Y/N)y\n"
          ]
        }
      ],
      "source": [
        "Fusion()\n",
        "df=res_fusion\n",
        "df_BackEnd=res_fusion_BackEnd\n",
        "df.createOrReplaceTempView(\"my_table\")\n",
        "df_BackEnd.createOrReplaceTempView(\"my_table_BackEnd\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs6kM8ISLeT_"
      },
      "source": [
        "### **Let's have a quick look at the Dataset :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUUtJsyetjXW",
        "outputId": "6ca117fd-90fb-4927-c655-558e12d23e82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+------+----------+------------+------------+--------+-----+\n",
            "|  PID|ST_NUM|   ST_NAME|OWN_OCCUPIED|NUM_BEDROOMS|NUM_BATH|SQ_FT|\n",
            "+-----+------+----------+------------+------------+--------+-----+\n",
            "|10001|   104|    PUTNAM|           Y|           3|       1| 1000|\n",
            "|10002|   197| LEXINGTON|           N|           3|     1.5| 1800|\n",
            "|10003|  TEST|      TEST|        TEST|        TEST|    TEST| TEST|\n",
            "|10004|   201|  BERKELEY|          12|           1|  HURLEY|  700|\n",
            "|10005|  TEST|      TEST|        TEST|        TEST|    TEST| TEST|\n",
            "|10006|   207|  BERKELEY|           Y|           3|       1|  800|\n",
            "|10007|   215|WASHINGTON|           Y|           2|  HURLEY|  950|\n",
            "|10008|   213|   TREMONT|           Y|           1|       1| 1800|\n",
            "|10009|  TEST|      TEST|        TEST|        TEST|    TEST| TEST|\n",
            "+-----+------+----------+------------+------------+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHB18ZcrWlAn"
      },
      "source": [
        "## **Traceability :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeYSHUChLqep"
      },
      "source": [
        "### **Let's look at the BackEnd Dataset :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYDCpVbyKxZr",
        "outputId": "d2454628-fcdb-4299-830d-e5846f2b5b88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+------+----------+------------+------------+--------+-----+----------+-------------------+\n",
            "|  PID|ST_NUM|   ST_NAME|OWN_OCCUPIED|NUM_BEDROOMS|NUM_BATH|SQ_FT|row_number|      modified_date|\n",
            "+-----+------+----------+------------+------------+--------+-----+----------+-------------------+\n",
            "|10001|   104|    PUTNAM|           Y|           3|       1| 1000|         1|2023-03-18 21:13:13|\n",
            "|10002|   197| LEXINGTON|           N|           3|     1.5| 1800|         2|2023-03-18 21:13:13|\n",
            "|10003|   215| LEXINGTON|           N|           3|       1|  850|         3|2023-03-18 21:13:13|\n",
            "|10003|  TEST|      TEST|        TEST|        TEST|    TEST| TEST|         4|2023-03-18 21:13:29|\n",
            "|10004|   201|  BERKELEY|          12|           1|  HURLEY|  700|         5|2023-03-18 21:13:13|\n",
            "|10005|   203|  BERKELEY|           Y|           3|       2| 1600|         6|2023-03-18 21:13:13|\n",
            "|10005|  TEST|      TEST|        TEST|        TEST|    TEST| TEST|         7|2023-03-18 21:13:42|\n",
            "|10006|   207|  BERKELEY|           Y|           3|       1|  800|         8|2023-03-18 21:13:13|\n",
            "|10007|   215|WASHINGTON|           Y|           2|  HURLEY|  950|         9|2023-03-18 21:13:13|\n",
            "|10008|   213|   TREMONT|           Y|           1|       1| 1800|        10|2023-03-18 21:13:13|\n",
            "|10009|   215|   TREMONT|           Y|           3|       2| 1800|        11|2023-03-18 21:13:13|\n",
            "|10009|  TEST|      TEST|        TEST|        TEST|    TEST| TEST|        12|2023-03-18 21:13:34|\n",
            "+-----+------+----------+------------+------------+--------+-----+----------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_BackEnd.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oBsLKBiV_yS",
        "outputId": "d8eda15e-1b5e-4803-8e4c-6dbf0b80cbf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+------+-------+------------+------------+--------+-----+----------+-------------------+\n",
            "|  PID|ST_NUM|ST_NAME|OWN_OCCUPIED|NUM_BEDROOMS|NUM_BATH|SQ_FT|row_number|      modified_date|\n",
            "+-----+------+-------+------------+------------+--------+-----+----------+-------------------+\n",
            "|10009|   215|TREMONT|           Y|           3|       2| 1800|        11|2023-03-18 21:13:13|\n",
            "|10009|  TEST|   TEST|        TEST|        TEST|    TEST| TEST|        12|2023-03-18 21:13:34|\n",
            "+-----+------+-------+------------+------------+--------+-----+----------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "request=spark.sql(\"select * from my_table_BackEnd where PID=10009\")\n",
        "request.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2iuwilMXuKN"
      },
      "source": [
        "# **Data Delivery :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "ia44kgihX1re",
        "outputId": "abe80bd2-680e-401c-82f9-e59551da1add"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "df_pandas=df.toPandas()\n",
        "\n",
        "# Save the Pandas DataFrame as a CSV file on your local machine\n",
        "df_pandas.to_csv(\"merged_dataset.csv\", index=False)\n",
        "\n",
        "# Download the CSV file to your local machine\n",
        "files.download(\"merged_dataset.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBAQteSDZzdR"
      },
      "source": [
        "# **SparkSQL :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4-zO4behSK4"
      },
      "source": [
        "### **Line Break function :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpKP3GeQhY8v"
      },
      "outputs": [],
      "source": [
        "def Line_Break(n):\n",
        "  rep=\"\"\n",
        "  for k in range(n):\n",
        "    rep+='\\n'\n",
        "  return rep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeNU-G1QgK1C"
      },
      "source": [
        "### **Automatized SparkSQL :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lr9AOzDmamJ4"
      },
      "outputs": [],
      "source": [
        "Queries=[]\n",
        "def SQL_Query():\n",
        "  rep=True\n",
        "  while(rep==True):\n",
        "    print(\"1-Make a query\")\n",
        "    print(\"2-Show result\")\n",
        "    print(\"3-Show all queries\")\n",
        "    print(\"4-Show a particular result\")\n",
        "    print(\"5-Leave\")\n",
        "    n=int(input(\"Make your choice : \"+Line_Break(2)))\n",
        "    if(n==1):\n",
        "      query=input(Line_Break(2)+\"Write your query : \"+Line_Break(2))\n",
        "      result = spark.sql(query)\n",
        "      Queries.append([query,result])\n",
        "    if(n==2):\n",
        "      print(Line_Break(2))\n",
        "      Queries[-1][1].show()\n",
        "      print(Line_Break(2))\n",
        "    if(n==3):\n",
        "      print(Line_Break(2))\n",
        "      for k in range(len(Queries)):\n",
        "        print(\"Query (ID=\"+str(k+1)+\") : \"+Queries[k][0],end=\"\\n\")\n",
        "        if(k==len(Queries)-1):\n",
        "          print(Line_Break(2))\n",
        "    if(n==4):\n",
        "      n1=int(input(\"\\n\\n\"+\"Type the query ID : \"+\"\\n\\n\"))\n",
        "      Queries[n1-1][1].show()\n",
        "    if(n==5):\n",
        "      rep=False\n",
        "SQL_Query()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
